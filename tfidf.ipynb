{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 手把手打一场NLP赛事"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "自然语言处理（Natural Language Processing，NLP）是指计算机处理和理解人类语言的技术。NLP涵盖了从文本的基本语法和词汇处理到更高级的任务，如机器翻译、情感分析、问答系统等。NLP利用计算机算法和模型，对文本进行分词、词性标注、句法分析等处理，以便将人类语言转化为计算机可以理解和处理的形式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 赛题信息\n",
    "医学领域的文献库中蕴含了丰富的疾病诊断和治疗信息，如何高效地从海量文献中提取关键信息，进行疾病诊断和治疗推荐，对于临床医生和研究人员具有重要意义。  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "基于论文摘要的文本分类与关键词抽取挑战赛  \n",
    "https://challenge.xfyun.cn/topic/info?type=abstract-of-the-paper&ch=ZuoaKcY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://ai-studio-static-online.cdn.bcebos.com/bc8c545638eb4200a68836ed741b6fe7d75108e9009d443b8de5b33fb8e0fa55)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.准备步骤"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 赛事报名\n",
    "赛事地址：https://challenge.xfyun.cn/topic/info?type=abstract-of-the-paper&ch=ZuoaKcY\n",
    "1. 点击报名参赛，登录讯飞开放平台。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 数据下载\n",
    "数据已提前下载在数据集目录下，您可以自行查看其中的train与test文件"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 环境配置参考资料\n",
    "python环境的搭建请参考：\n",
    "- [Mac设备：Mac上安装Anaconda最全教程](https://zhuanlan.zhihu.com/p/350828057)\n",
    "- [Windows设备：Anaconda超详细安装教程(Windows环境下)_菜鸟1号!!的博客-CSDN博客_windows安装anaconda](https://blog.csdn.net/fan18317517352/article/details/123035625)\n",
    "Jupyter 编辑器的使用请参考：\n",
    "- [Jupyter Notebook最全使用教程，看这篇就够了！](https://www.jianshu.com/p/6cc047bc94e5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 赛题解析\n",
    "实践任务\n",
    "本任务分为两个子任务：\n",
    "1. 从论文标题、摘要作者等信息，判断该论文是否属于医学领域的文献。\n",
    "2. 从论文标题、摘要作者等信息，提取出该论文关键词。\n",
    "\n",
    "第一个任务看作是一个文本二分类任务。机器需要根据对论文摘要等信息的理解，将论文划分为医学领域的文献和非医学领域的文献两个类别之一。第二个任务看作是一个文本关键词识别任务。机器需要从给定的论文中识别和提取出与论文内容相关的关键词。\n",
    "\n",
    "数据集解析\n",
    "训练集与测试集数据为CSV格式文件，各字段分别是标题、作者和摘要。Keywords为任务2的标签，label为任务1的标签。训练集和测试集都可以通过pandas读取。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://ai-studio-static-online.cdn.bcebos.com/8c88537bce9d46049151389396c4c5b828556ba332d34ed3a24555e4e28e7191)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.实践思路&baseline\n",
    "### 实践思路\n",
    "本赛题可以分为两个子任务：\n",
    "1. 从论文标题、摘要作者等信息，判断该论文是否属于医学领域的文献。\n",
    "2. 从论文标题、摘要作者等信息，提取出该论文关键词。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 任务一：文本二分类\n",
    "第一个任务看作是一个文本二分类任务。机器需要根据对论文摘要等信息的理解，将论文划分为医学领域的文献和非医学领域的文献两个类别之一。\n",
    "\n",
    "针对文本分类任务，可以提供两种实践思路，一种是使用传统的特征提取方法（如TF-IDF/BOW）结合机器学习模型，另一种是使用预训练的BERT模型进行建模。使用特征提取 + 机器学习的思路步骤如下：\n",
    "1. 数据预处理：首先，对文本数据进行预处理，包括文本清洗（如去除特殊字符、标点符号）、分词等操作。可以使用常见的NLP工具包（如NLTK或spaCy）来辅助进行预处理。\n",
    "2. 特征提取：使用TF-IDF（词频-逆文档频率）或BOW（词袋模型）方法将文本转换为向量表示。TF-IDF可以计算文本中词语的重要性，而BOW则简单地统计每个词语在文本中的出现次数。可以使用scikit-learn库的TfidfVectorizer或CountVectorizer来实现特征提取。\n",
    "3. 构建训练集和测试集：将预处理后的文本数据分割为训练集和测试集，确保数据集的样本分布均匀。\n",
    "4. 选择机器学习模型：根据实际情况选择适合的机器学习模型，如朴素贝叶斯、支持向量机（SVM）、随机森林等。这些模型在文本分类任务中表现良好。可以使用scikit-learn库中相应的分类器进行模型训练和评估。\n",
    "5. 模型训练和评估：使用训练集对选定的机器学习模型进行训练，然后使用测试集进行评估。评估指标可以选择准确率、精确率、召回率、F1值等。\n",
    "6. 调参优化：如果模型效果不理想，可以尝试调整特征提取的参数（如词频阈值、词袋大小等）或机器学习模型的参数，以获得更好的性能。  \n",
    "\n",
    "\n",
    "Baseline中我们选择使用BOW将文本转换为向量表示，选择逻辑回归模型来完成训练和评估  \n",
    "代码演示如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-19T12:52:19.549752Z",
     "iopub.status.busy": "2023-07-19T12:52:19.548787Z",
     "iopub.status.idle": "2023-07-19T12:52:23.377489Z",
     "shell.execute_reply": "2023-07-19T12:52:23.376535Z",
     "shell.execute_reply.started": "2023-07-19T12:52:19.549712Z"
    },
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2023-07-24T04:03:43.836216Z",
     "start_time": "2023-07-24T04:03:42.831365Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /Users/gwj1139/anaconda3/envs/nlp_whale/lib/python3.10/site-packages (3.8.1)\r\n",
      "Requirement already satisfied: click in /Users/gwj1139/anaconda3/envs/nlp_whale/lib/python3.10/site-packages (from nltk) (8.1.6)\r\n",
      "Requirement already satisfied: joblib in /Users/gwj1139/anaconda3/envs/nlp_whale/lib/python3.10/site-packages (from nltk) (1.3.1)\r\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/gwj1139/anaconda3/envs/nlp_whale/lib/python3.10/site-packages (from nltk) (2023.6.3)\r\n",
      "Requirement already satisfied: tqdm in /Users/gwj1139/anaconda3/envs/nlp_whale/lib/python3.10/site-packages (from nltk) (4.65.0)\r\n"
     ]
    }
   ],
   "source": [
    "# 获取前置依赖\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-07-20T04:09:20.317992Z",
     "iopub.status.busy": "2023-07-20T04:09:20.317403Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true,
    "tags": [],
    "ExecuteTime": {
     "end_time": "2023-07-24T04:15:09.356875Z",
     "start_time": "2023-07-24T04:15:07.441858Z"
    }
   },
   "outputs": [],
   "source": [
    "# 导入pandas用于读取表格数据\n",
    "import pandas as pd\n",
    "\n",
    "# 导入BOW（词袋模型），可以选择将CountVectorizer替换为TfidfVectorizer（TF-IDF（词频-逆文档频率）），注意上下文要同时修改，亲测后者效果更佳\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# 导入LogisticRegression回归模型\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# 过滤警告消息\n",
    "from warnings import simplefilter\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "simplefilter(\"ignore\", category=ConvergenceWarning)\n",
    "\n",
    "\n",
    "# 读取数据集\n",
    "train = pd.read_csv('data/train.csv')\n",
    "train['title'] = train['title'].fillna('')\n",
    "train['abstract'] = train['abstract'].fillna('')\n",
    "\n",
    "test = pd.read_csv('data/testB.csv')\n",
    "test['title'] = test['title'].fillna('')\n",
    "test['abstract'] = test['abstract'].fillna('')\n",
    "\n",
    "\n",
    "# 提取文本特征，生成训练集与测试集\n",
    "train['text'] = train['title'].fillna('') + ' ' +  train['author'].fillna('') + ' ' + train['abstract'].fillna('')+ ' ' + train['Keywords'].fillna('')\n",
    "test['text'] = test['title'].fillna('') + ' ' +  test['author'].fillna('') + ' ' + test['abstract'].fillna('')+ ' ' + train['Keywords'].fillna('')\n",
    "\n",
    "vector = CountVectorizer().fit(train['text'])\n",
    "train_vector = vector.transform(train['text'])\n",
    "test_vector = vector.transform(test['text'])\n",
    "\n",
    "\n",
    "# 引入模型\n",
    "model = LogisticRegression()\n",
    "\n",
    "# 开始训练，这里可以考虑修改默认的batch_size与epoch来取得更好的效果\n",
    "model.fit(train_vector, train['label'])\n",
    "\n",
    "# 利用模型对测试集label标签进行预测\n",
    "test['label'] = model.predict(test_vector)\n",
    "\n",
    "# 生成任务一推测结果\n",
    "# test[['uuid', 'Keywords', 'label']].to_csv('submit_task1.csv', index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 任务二：关键词提取\n",
    "论文关键词划分为两类：\n",
    "- 在标题和摘要中出现的关键词\n",
    "- 没有在标题和摘要中出的关键词\n",
    "\n",
    "在标题和摘要中出现的关键词：这些关键词是文本的核心内容，通常在文章的标题和摘要中出现，用于概括和提炼文本的主题或要点。对于提取这类关键词，可以采用以下方法：\n",
    "  - 词频统计：统计标题和摘要中的词频，选择出现频率较高的词语作为关键词。同时设置停用词去掉价值不大、有负作用的词语。\n",
    "  - 词性过滤：根据文本的词性信息，筛选出名词、动词、形容词等词性的词语作为关键词。\n",
    "  - TF-IDF算法：计算词语在文本中的词频和逆文档频率，选择TF-IDF值较高的词语作为关键词。\n",
    "\n",
    "没有在标题和摘要中出现的关键词：这类关键词可能在文本的正文部分出现，但并没有在标题和摘要中提及。要提取这些关键词，可以考虑以下方法：\n",
    "  - 文本聚类：将文本划分为不同的主题或类别，提取每个主题下的关键词。\n",
    "  - 上下文分析：通过分析关键词周围的上下文信息，判断其重要性和相关性。\n",
    "  - 基于机器学习/深度学习的方法：使用监督学习或无监督学习的方法训练模型，从文本中提取出未出现在标题和摘要中的关键词。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-07-19T12:35:52.426781Z",
     "iopub.status.busy": "2023-07-19T12:35:52.426142Z",
     "iopub.status.idle": "2023-07-19T12:36:04.557644Z",
     "shell.execute_reply": "2023-07-19T12:36:04.556712Z",
     "shell.execute_reply.started": "2023-07-19T12:35:52.426750Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 44\u001B[0m\n\u001B[1;32m     41\u001B[0m \u001B[38;5;66;03m## 对测试集提取关键词   \u001B[39;00m\n\u001B[1;32m     43\u001B[0m test_words \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m---> 44\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m row \u001B[38;5;129;01min\u001B[39;00m \u001B[43mtest\u001B[49m\u001B[38;5;241m.\u001B[39miterrows():\n\u001B[1;32m     45\u001B[0m     \u001B[38;5;66;03m# 读取第每一行数据的标题与摘要并提取关键词\u001B[39;00m\n\u001B[1;32m     46\u001B[0m     prediction_keywords \u001B[38;5;241m=\u001B[39m extract_keywords_by_freq(row[\u001B[38;5;241m1\u001B[39m]\u001B[38;5;241m.\u001B[39mtitle, row[\u001B[38;5;241m1\u001B[39m]\u001B[38;5;241m.\u001B[39mabstract)\n\u001B[1;32m     47\u001B[0m     \u001B[38;5;66;03m# 利用文章标题进一步提取关键词\u001B[39;00m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'test' is not defined"
     ]
    }
   ],
   "source": [
    "# 引入分词器\n",
    "from nltk import word_tokenize, ngrams\n",
    "\n",
    "# 定义停用词，去掉出现较多，但对文章不关键的词语\n",
    "stops = [\n",
    "    'will', 'can', \"couldn't\", 'same', 'own', \"needn't\", 'between', \"shan't\", 'very',\n",
    "     'so', 'over', 'in', 'have', 'the', 's', 'didn', 'few', 'should', 'of', 'that', \n",
    "     'don', 'weren', 'into', \"mustn't\", 'other', 'from', \"she's\", 'hasn', \"you're\",\n",
    "     'ain', 'ours', 'them', 'he', 'hers', 'up', 'below', 'won', 'out', 'through',\n",
    "     'than', 'this', 'who', \"you've\", 'on', 'how', 'more', 'being', 'any', 'no',\n",
    "     'mightn', 'for', 'again', 'nor', 'there', 'him', 'was', 'y', 'too', 'now',\n",
    "     'whom', 'an', 've', 'or', 'itself', 'is', 'all', \"hasn't\", 'been', 'themselves',\n",
    "     'wouldn', 'its', 'had', \"should've\", 'it', \"you'll\", 'are', 'be', 'when', \"hadn't\",\n",
    "     \"that'll\", 'what', 'while', 'above', 'such', 'we', 't', 'my', 'd', 'i', 'me',\n",
    "     'at', 'after', 'am', 'against', 'further', 'just', 'isn', 'haven', 'down',\n",
    "     \"isn't\", \"wouldn't\", 'some', \"didn't\", 'ourselves', 'their', 'theirs', 'both',\n",
    "     're', 'her', 'ma', 'before', \"don't\", 'having', 'where', 'shouldn', 'under',\n",
    "     'if', 'as', 'myself', 'needn', 'these', 'you', 'with', 'yourself', 'those',\n",
    "     'each', 'herself', 'off', 'to', 'not', 'm', \"it's\", 'does', \"weren't\", \"aren't\",\n",
    "     'were', 'aren', 'by', 'doesn', 'himself', 'wasn', \"you'd\", 'once', 'because', 'yours',\n",
    "     'has', \"mightn't\", 'they', 'll', \"haven't\", 'but', 'couldn', 'a', 'do', 'hadn',\n",
    "     \"doesn't\", 'your', 'she', 'yourselves', 'o', 'our', 'here', 'and', 'his', 'most',\n",
    "     'about', 'shan', \"wasn't\", 'then', 'only', 'mustn', 'doing', 'during', 'why',\n",
    "     \"won't\", 'until', 'did', \"shouldn't\", 'which'\n",
    "]\n",
    "\n",
    "# 定义方法按照词频筛选关键词\n",
    "\n",
    "def extract_keywords_by_freq(title, abstract):\n",
    "    ngrams_count = list(ngrams(word_tokenize(title.lower()), 2)) + list(ngrams(word_tokenize(abstract.lower()), 2))\n",
    "    ngrams_count = pd.DataFrame(ngrams_count)\n",
    "    ngrams_count = ngrams_count[~ngrams_count[0].isin(stops)]\n",
    "    ngrams_count = ngrams_count[~ngrams_count[1].isin(stops)]\n",
    "    ngrams_count = ngrams_count[ngrams_count[0].apply(len) > 3]\n",
    "    ngrams_count = ngrams_count[ngrams_count[1].apply(len) > 3]\n",
    "    ngrams_count['phrase'] = ngrams_count[0] + ' ' + ngrams_count[1]\n",
    "    ngrams_count = ngrams_count['phrase'].value_counts()\n",
    "    ngrams_count = ngrams_count[ngrams_count > 1]\n",
    "    return list(ngrams_count.index)[:6]\n",
    "\n",
    "## 对测试集提取关键词   \n",
    "\n",
    "test_words = []\n",
    "for row in test.iterrows():\n",
    "    # 读取第每一行数据的标题与摘要并提取关键词\n",
    "    prediction_keywords = extract_keywords_by_freq(row[1].title, row[1].abstract)\n",
    "    # 利用文章标题进一步提取关键词\n",
    "    prediction_keywords = [x.title() for x in prediction_keywords]\n",
    "    # 如果未能提取到关键词\n",
    "    if len(prediction_keywords) == 0:\n",
    "        prediction_keywords = ['A', 'B']\n",
    "    test_words.append('; '.join(prediction_keywords))\n",
    "    \n",
    "test['Keywords'] = test_words\n",
    "test[['uuid', 'Keywords', 'label']].to_csv('submit_task2.csv', index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "完整baseline运行后a榜分数在0.97655左右"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "请点击[此处](https://ai.baidu.com/docs#/AIStudio_Project_Notebook/a38e5576)查看本环境基本用法.  <br>\n",
    "Please click [here ](https://ai.baidu.com/docs#/AIStudio_Project_Notebook/a38e5576) for more detailed instructions. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "toc-autonumbering": false,
  "toc-showcode": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
