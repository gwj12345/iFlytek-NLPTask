{
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.13",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "tianchi_metadata": {
   "competitions": [],
   "datasets": [],
   "description": "",
   "notebookId": "315892",
   "source": "dsw"
  },
  "toc-autonumbering": false
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "#导入前置依赖\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "# 用于加载bert模型的分词器\n",
    "from transformers import AutoTokenizer\n",
    "# 用于加载bert模型\n",
    "from transformers import BertModel\n",
    "from pathlib import Path\n",
    "\n"
   ],
   "metadata": {
    "trusted": true,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2023-07-20T14:31:41.825584Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "batch_size = 16\n",
    "# 文本的最大长度\n",
    "text_max_length = 128\n",
    "# 总训练的epochs数，我只是随便定义了个数\n",
    "epochs = 100\n",
    "# 学习率\n",
    "lr = 3e-5\n",
    "# 取多少训练集的数据作为验证集\n",
    "validation_ratio = 0.1\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# 每多少步，打印一次loss\n",
    "log_per_step = 50\n",
    "\n",
    "# 数据集所在位置\n",
    "dataset_dir = Path(\"./data\")\n",
    "os.makedirs(dataset_dir) if not os.path.exists(dataset_dir) else ''\n",
    "\n",
    "# 模型存储路径\n",
    "model_dir = Path(\"./model/bert_checkpoints\")\n",
    "# 如果模型目录不存在，则创建一个\n",
    "os.makedirs(model_dir) if not os.path.exists(model_dir) else ''\n",
    "\n",
    "print(\"Device:\", device)\n",
    "\n"
   ],
   "metadata": {
    "trusted": true,
    "is_executing": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# 读取数据集，进行数据处理\n",
    "\n",
    "pd_train_data = pd.read_csv('./data/train.csv')\n",
    "pd_train_data['title'] = pd_train_data['title'].fillna('')\n",
    "pd_train_data['abstract'] = pd_train_data['abstract'].fillna('')\n",
    "\n",
    "test_data = pd.read_csv('./data/test.csv')\n",
    "test_data['title'] = test_data['title'].fillna('')\n",
    "test_data['abstract'] = test_data['abstract'].fillna('')\n",
    "pd_train_data['text'] = pd_train_data['title'].fillna('') + ' ' +  pd_train_data['author'].fillna('') + ' ' + pd_train_data['abstract'].fillna('')+ ' ' + pd_train_data['Keywords'].fillna('')\n",
    "test_data['text'] = test_data['title'].fillna('') + ' ' +  test_data['author'].fillna('') + ' ' + test_data['abstract'].fillna('')+ ' ' + pd_train_data['Keywords'].fillna('')\n"
   ],
   "metadata": {
    "trusted": true,
    "is_executing": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# 从训练集中随机采样测试集\n",
    "validation_data = pd_train_data.sample(frac=validation_ratio)\n",
    "train_data = pd_train_data[~pd_train_data.index.isin(validation_data.index)]"
   ],
   "metadata": {
    "trusted": true,
    "is_executing": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# 构建Dataset\n",
    "class MyDataset(Dataset):\n",
    "\n",
    "    def __init__(self, mode='train'):\n",
    "        super(MyDataset, self).__init__()\n",
    "        self.mode = mode\n",
    "        # 拿到对应的数据\n",
    "        if mode == 'train':\n",
    "            self.dataset = train_data\n",
    "        elif mode == 'validation':\n",
    "            self.dataset = validation_data\n",
    "        elif mode == 'test':\n",
    "            # 如果是测试模式，则返回内容和uuid。拿uuid做target主要是方便后面写入结果。\n",
    "            self.dataset = test_data\n",
    "        else:\n",
    "            raise Exception(\"Unknown mode {}\".format(mode))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # 取第index条\n",
    "        data = self.dataset.iloc[index]\n",
    "        # 取其内容\n",
    "        text = data['text']\n",
    "        # 根据状态返回内容\n",
    "        if self.mode == 'test':\n",
    "            # 如果是test，将uuid做为target\n",
    "            label = data['uuid']\n",
    "        else:\n",
    "            label = data['label']\n",
    "        # 返回内容和label\n",
    "        return text, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n"
   ],
   "metadata": {
    "trusted": true,
    "is_executing": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "train_dataset = MyDataset('train')\n",
    "validation_dataset = MyDataset('validation')"
   ],
   "metadata": {
    "trusted": true,
    "is_executing": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "train_dataset.__getitem__(0)\n"
   ],
   "metadata": {
    "trusted": true,
    "is_executing": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#获取Bert预训练模型\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")"
   ],
   "metadata": {
    "trusted": true,
    "is_executing": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#接着构造我们的Dataloader。\n",
    "#我们需要定义一下collate_fn，在其中完成对句子进行编码、填充、组装batch等动作：\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    将一个batch的文本句子转成tensor，并组成batch。\n",
    "    :param batch: 一个batch的句子，例如: [('推文', target), ('推文', target), ...]\n",
    "    :return: 处理后的结果，例如：\n",
    "             src: {'input_ids': tensor([[ 101, ..., 102, 0, 0, ...], ...]), 'attention_mask': tensor([[1, ..., 1, 0, ...], ...])}\n",
    "             target：[1, 1, 0, ...]\n",
    "    \"\"\"\n",
    "    text, label = zip(*batch)\n",
    "    text, label = list(text), list(label)\n",
    "\n",
    "    # src是要送给bert的，所以不需要特殊处理，直接用tokenizer的结果即可\n",
    "    # padding='max_length' 不够长度的进行填充\n",
    "    # truncation=True 长度过长的进行裁剪\n",
    "    src = tokenizer(text, padding='max_length', max_length=text_max_length, return_tensors='pt', truncation=True)\n",
    "\n",
    "    return src, torch.LongTensor(label)"
   ],
   "metadata": {
    "trusted": true,
    "is_executing": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)"
   ],
   "metadata": {
    "trusted": true,
    "is_executing": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "inputs, targets = next(iter(train_loader))\n",
    "print(\"inputs:\", inputs)\n",
    "print(\"targets:\", targets)"
   ],
   "metadata": {
    "trusted": true,
    "is_executing": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "#定义预测模型，该模型由bert模型加上最后的预测层组成\n",
    "class MyModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "\n",
    "        # 加载bert模型\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased', mirror='tuna')\n",
    "\n",
    "        # 最后的预测层\n",
    "        self.predictor = nn.Sequential(\n",
    "            nn.Linear(768, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, src):\n",
    "        \"\"\"\n",
    "        :param src: 分词后的推文数据\n",
    "        \"\"\"\n",
    "\n",
    "        # 将src直接序列解包传入bert，因为bert和tokenizer是一套的，所以可以这么做。\n",
    "        # 得到encoder的输出，用最前面[CLS]的输出作为最终线性层的输入\n",
    "        outputs = self.bert(**src).last_hidden_state[:, 0, :]\n",
    "\n",
    "        # 使用线性层来做最终的预测\n",
    "        return self.predictor(outputs)\n"
   ],
   "metadata": {
    "trusted": true,
    "is_executing": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "model = MyModel()\n",
    "model = model.to(device)"
   ],
   "metadata": {
    "trusted": true,
    "is_executing": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "#定义出损失函数和优化器。这里使用Binary Cross Entropy：\n",
    "criteria = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n"
   ],
   "metadata": {
    "trusted": true,
    "is_executing": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# 由于inputs是字典类型的，定义一个辅助函数帮助to(device)\n",
    "def to_device(dict_tensors):\n",
    "    result_tensors = {}\n",
    "    for key, value in dict_tensors.items():\n",
    "        result_tensors[key] = value.to(device)\n",
    "    return result_tensors"
   ],
   "metadata": {
    "trusted": true,
    "is_executing": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "#定义一个验证方法，获取到验证集的精准率和loss\n",
    "def validate():\n",
    "    model.eval()\n",
    "    total_loss = 0.\n",
    "    total_correct = 0\n",
    "    for inputs, targets in validation_loader:\n",
    "        inputs, targets = to_device(inputs), targets.to(device)\n",
    "        outputs = model(inputs)\n",
    "        loss = criteria(outputs.view(-1), targets.float())\n",
    "        total_loss += float(loss)\n",
    "\n",
    "        correct_num = (((outputs >= 0.5).float() * 1).flatten() == targets).sum()\n",
    "        total_correct += correct_num\n",
    "\n",
    "    return total_correct / len(validation_dataset), total_loss / len(validation_dataset)\n"
   ],
   "metadata": {
    "trusted": true,
    "is_executing": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# 首先将模型调成训练模式\n",
    "model.train()\n",
    "\n",
    "# 清空一下cuda缓存\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# 定义几个变量，帮助打印loss\n",
    "total_loss = 0.\n",
    "# 记录步数\n",
    "step = 0\n",
    "\n",
    "# 记录在验证集上最好的准确率\n",
    "best_accuracy = 0\n",
    "\n",
    "# 开始训练\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    for i, (inputs, targets) in enumerate(train_loader):\n",
    "        # 从batch中拿到训练数据\n",
    "        inputs, targets = to_device(inputs), targets.to(device)\n",
    "        # 传入模型进行前向传递\n",
    "        outputs = model(inputs)\n",
    "        # 计算损失\n",
    "        loss = criteria(outputs.view(-1), targets.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        total_loss += float(loss)\n",
    "        step += 1\n",
    "\n",
    "        if step % log_per_step == 0:\n",
    "            print(\"Epoch {}/{}, Step: {}/{}, total loss:{:.4f}\".format(epoch+1, epochs, i, len(train_loader), total_loss))\n",
    "            total_loss = 0\n",
    "\n",
    "        del inputs, targets\n",
    "\n",
    "    # 一个epoch后，使用过验证集进行验证\n",
    "    accuracy, validation_loss = validate()\n",
    "    print(\"Epoch {}, accuracy: {:.4f}, validation loss: {:.4f}\".format(epoch+1, accuracy, validation_loss))\n",
    "    torch.save(model, model_dir / f\"model_{epoch}.pt\")\n",
    "\n",
    "    # 保存最好的模型\n",
    "    if accuracy > best_accuracy:\n",
    "        torch.save(model, model_dir / f\"model_best.pt\")\n",
    "        best_accuracy = accuracy"
   ],
   "metadata": {
    "trusted": true,
    "is_executing": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "#加载最好的模型，然后进行测试集的预测\n",
    "model = torch.load(model_dir / f\"model_best.pt\")\n",
    "model = model.eval()\n"
   ],
   "metadata": {
    "trusted": true,
    "is_executing": true
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "test_dataset = MyDataset('test')\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)"
   ],
   "metadata": {
    "trusted": true,
    "is_executing": true
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "results = []\n",
    "for inputs, ids in test_loader:\n",
    "    outputs = model(inputs.to(device))\n",
    "    outputs = (outputs >= 0.5).int().flatten().tolist()\n",
    "    ids = ids.tolist()\n",
    "    results = results + [(id, result) for result, id in zip(outputs, ids)]\n",
    "    "
   ],
   "metadata": {
    "trusted": true,
    "is_executing": true
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "test_label = [pair[1] for pair in results]\n",
    "test_data['label'] = test_label\n",
    "test_data[['uuid', 'Keywords', 'label']].to_csv('submit_task1.csv', index=None)\n",
    "\n"
   ],
   "metadata": {
    "trusted": true,
    "is_executing": true
   },
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Task2\n",
    "\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# 导入pandas用于读取表格数据\n",
    "import pandas as pd\n",
    "\n",
    "# 导入BOW（词袋模型），可以选择将CountVectorizer替换为TfidfVectorizer（TF-IDF（词频-逆文档频率）），注意上下文要同时修改，亲测后者效果更佳\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# 导入Bert模型\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# 导入计算相似度前置库，为了计算候选者和文档之间的相似度，我们将使用向量之间的余弦相似度，因为它在高维度下表现得相当好。\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# 过滤警告消息\n",
    "from warnings import simplefilter\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "simplefilter(\"ignore\", category=ConvergenceWarning)\n"
   ],
   "metadata": {
    "trusted": true,
    "is_executing": true
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# 读取数据集\n",
    "test = pd.read_csv('./data/test.csv')\n",
    "test['title'] = test['title'].fillna('')\n",
    "test['abstract'] = test['abstract'].fillna('')\n",
    "\n",
    "test['text'] = test['title'].fillna('') + ' ' +test['abstract'].fillna('')\n"
   ],
   "metadata": {
    "trusted": true,
    "is_executing": true
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "# 定义停用词，去掉出现较多，但对文章不关键的词语\n",
    "stops =[i.strip() for i in open(r'stop.txt',encoding='utf-8').readlines()] \n"
   ],
   "metadata": {
    "trusted": true,
    "is_executing": true
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "model = SentenceTransformer(r'xlm-r-distilroberta-base-paraphrase-v1')"
   ],
   "metadata": {
    "trusted": true,
    "is_executing": true
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "test_words = []\n",
    "for row in test.iterrows():\n",
    "    # 读取第每一行数据的标题与摘要并提取关键词\n",
    "    # 修改n_gram_range来改变结果候选词的词长大小。例如，如果我们将它设置为(3，3)，那么产生的候选词将是包含3个关键词的短语。\n",
    "    n_gram_range = (1,1)\n",
    "    # 这里我们使用TF-IDF算法来获取候选关键词 \n",
    "    count = TfidfVectorizer(ngram_range=n_gram_range, stop_words=stops).fit([row[1].text])\n",
    "    candidates = count.get_feature_names_out()\n",
    "    # 将文本标题以及候选关键词/关键短语转换为数值型数据（numerical data）。我们使用BERT来实现这一目的\n",
    "    title_embedding = model.encode([row[1].title])\n",
    "    \n",
    "    candidate_embeddings = model.encode(candidates)\n",
    "    \n",
    "    # 通过修改这个参数来更改关键词数量\n",
    "    top_n = 15\n",
    "    # 利用文章标题进一步提取关键词\n",
    "    distances = cosine_similarity(title_embedding, candidate_embeddings)\n",
    "    keywords = [candidates[index] for index in distances.argsort()[0][-top_n:]]\n",
    "    \n",
    "    if len( keywords) == 0:\n",
    "         keywords = ['A', 'B']\n",
    "    test_words.append('; '.join( keywords))\n",
    "    "
   ],
   "metadata": {
    "trusted": true,
    "is_executing": true
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "test['Keywords'] = test_words\n",
    "test[['uuid', 'Keywords']].to_csv('submit_task2.csv', index=None)"
   ],
   "metadata": {
    "trusted": true,
    "is_executing": true
   },
   "outputs": []
  }
 ]
}
